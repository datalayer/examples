{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction tuning Mistral 7B on Alpaca dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to fine-tune [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) using the [Alapca dataset](https://huggingface.co/datasets/tatsu-lab/alpaca). Mistral 7B is a large language model (LLM) that contains 7.3 billion parameters and is one of the most powerful models for its size. However, this base model is not instruction-tuned, meaning it may struggle to follow instructions and perform specific tasks.\n",
    "\n",
    "The Alpaca dataset consists of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine. These can be used for instruction tuning, helping language models to better understand and follow instructions. By fine-tuning Mistral 7B on the Alpaca dataset, the model will significantly improve its capabilities to perform tasks such as conversation and answering questions accurately.\n",
    "\n",
    "We will utilize [torchtune](https://github.com/pytorch/torchtune), a PyTorch-native library designed to facilitate experimentation with LLMs, for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T14:24:16.552945Z",
     "iopub.status.busy": "2024-06-26T14:24:16.551743Z",
     "iopub.status.idle": "2024-06-26T14:24:25.605472Z",
     "shell.execute_reply": "2024-06-26T14:24:25.604084Z",
     "shell.execute_reply.started": "2024-06-26T14:24:16.552891Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.2.2 in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (4.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.2.2) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch==2.2.2) (1.3.0)\n",
      "Collecting torchtune\n",
      "  Downloading torchtune-0.1.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting datasets (from torchtune)\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting huggingface-hub (from torchtune)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting safetensors (from torchtune)\n",
      "  Downloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentencepiece (from torchtune)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from torchtune)\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting blobfile>=2 (from torchtune)\n",
      "  Downloading blobfile-2.1.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from torchtune) (4.66.2)\n",
      "Collecting omegaconf (from torchtune)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting torchao==0.1 (from torchtune)\n",
      "  Downloading torchao-0.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from torchao==0.1->torchtune) (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchao==0.1->torchtune) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from torchao==0.1->torchtune) (24.0)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in /opt/conda/lib/python3.11/site-packages (from blobfile>=2->torchtune) (3.20.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /opt/conda/lib/python3.11/site-packages (from blobfile>=2->torchtune) (2.2.1)\n",
      "Collecting lxml~=4.9 (from blobfile>=2->torchtune)\n",
      "  Downloading lxml-4.9.4-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: filelock~=3.0 in /opt/conda/lib/python3.11/site-packages (from blobfile>=2->torchtune) (3.13.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (15.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (2.2.1)\n",
      "Collecting requests>=2.32.2 (from datasets->torchtune)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from torchtune)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets->torchtune)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->torchtune)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->torchtune) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->torchtune) (4.10.0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->torchtune)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken->torchtune)\n",
      "  Downloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->torchtune) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->torchtune) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->torchtune) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->torchtune) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->torchtune) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets->torchtune) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets->torchtune) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets->torchtune) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->torchtune) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->torchtune) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->torchtune) (2024.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchao==0.1->torchtune) (12.4.127)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->torchao==0.1->torchtune) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch->torchao==0.1->torchtune) (1.3.0)\n",
      "Downloading torchtune-0.1.1-py3-none-any.whl (210 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchao-0.1-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blobfile-2.1.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-4.9.4-cp311-cp311-manylinux_2_28_x86_64.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m131.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.0/785.0 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=cbe95de22700a5801c6c77c11cca0c9c2bb4192b4ca5ba05e023c65a92d450ba\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: sentencepiece, antlr4-python3-runtime, xxhash, tqdm, safetensors, requests, regex, omegaconf, multiprocess, lxml, tiktoken, huggingface-hub, blobfile, torchao, datasets, torchtune\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 blobfile-2.1.1 datasets-2.20.0 huggingface-hub-0.23.4 lxml-4.9.4 multiprocess-0.70.16 omegaconf-2.3.0 regex-2024.5.15 requests-2.32.3 safetensors-0.4.3 sentencepiece-0.2.0 tiktoken-0.7.0 torchao-0.1 torchtune-0.1.1 tqdm-4.66.4 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install torch==2.2.2\n",
    "! pip install torchtune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T14:24:25.609414Z",
     "iopub.status.busy": "2024-06-26T14:24:25.608395Z",
     "iopub.status.idle": "2024-06-26T14:24:25.939574Z",
     "shell.execute_reply": "2024-06-26T14:24:25.938226Z",
     "shell.execute_reply.started": "2024-06-26T14:24:25.609357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 26 14:24:25 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100S-PCIE-32GB          On  |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   44C    P0             28W /  250W |       0MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPU check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Mistral 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to donwload Mistral 7B. This can be achieved through torchtune with the following cell.\n",
    "\n",
    "> **_NOTE:_** Set your environment variable `<HF_TOKEN>` or pass in --hf-token to the command in order to validate your access. You can find your token at https://huggingface.co/settings/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T14:24:25.941920Z",
     "iopub.status.busy": "2024-06-26T14:24:25.941414Z",
     "iopub.status.idle": "2024-06-26T14:24:30.298609Z",
     "shell.execute_reply": "2024-06-26T14:24:30.297075Z",
     "shell.execute_reply.started": "2024-06-26T14:24:25.941863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: *.safetensors\n",
      "usage: tune download <repo-id> [OPTIONS]\n",
      "tune download: error: An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the specified revision on the local disk. Please check your internet connection and try again.\n"
     ]
    }
   ],
   "source": [
    "! tune download mistralai/Mistral-7B-v0.1 \\\n",
    "--output-dir ./mistral-7B \\\n",
    "--hf-token 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 #--hf-token <HF_TOKEN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using Mistral 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've downloaded the base model, let's employ it to generate an answer from a text input.\n",
    "\n",
    "When using torchtune, several pieces of information need to be provided in a configuration file, including the type of model to use, its location, and which type of device should be utilized. The cell below generates a *.yaml* file containing all the necessary information to use Mistral 7B for inference on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T14:24:30.302341Z",
     "iopub.status.busy": "2024-06-26T14:24:30.301043Z",
     "iopub.status.idle": "2024-06-26T14:24:30.311009Z",
     "shell.execute_reply": "2024-06-26T14:24:30.309968Z",
     "shell.execute_reply.started": "2024-06-26T14:24:30.302279Z"
    }
   },
   "outputs": [],
   "source": [
    "# mistral_generation.yaml\n",
    "\n",
    "with open(\"mistral_generation.yaml\", \"w\") as fp:\n",
    "    fp.write(\n",
    "        \"\"\"\n",
    "        # Config for running the InferenceRecipe in generate.py to generate output from an LLM\n",
    "        #\n",
    "        # To launch, run the following command from root torchtune directory:\n",
    "        #    tune run generate --config generation\n",
    "        \n",
    "        # Model arguments\n",
    "        model:\n",
    "          _component_: torchtune.models.mistral.mistral_7b\n",
    "        \n",
    "        checkpointer:\n",
    "          _component_: torchtune.utils.FullModelHFCheckpointer\n",
    "          checkpoint_dir: /home/jovyan/content/mistral-7B\n",
    "          checkpoint_files: [\n",
    "            pytorch_model-00001-of-00002.bin,\n",
    "            pytorch_model-00002-of-00002.bin\n",
    "          ]\n",
    "          recipe_checkpoint: null\n",
    "          output_dir: /home/jovyan/content/mistral-7B\n",
    "          model_type: MISTRAL\n",
    "        resume_from_checkpoint: False\n",
    "        \n",
    "        device: cuda\n",
    "        dtype: bf16\n",
    "        \n",
    "        seed: 1234\n",
    "        \n",
    "        # Tokenizer arguments\n",
    "        tokenizer:\n",
    "          _component_: torchtune.models.mistral.mistral_tokenizer\n",
    "          path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
    "        \n",
    "        # Generation arguments; defaults taken from gpt-fast\n",
    "        prompt: \"Hello, my name is\"\n",
    "        max_new_tokens: 300\n",
    "        temperature: 0.6 # 0.8 and 0.6 are popular values to try\n",
    "        top_k: 300\n",
    "        \n",
    "        quantizer: null\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to get an answer from Mistral 7B to the following prompt:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "### Input:\n",
    "Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n",
    "\n",
    "### Context:\n",
    "CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T14:24:30.313728Z",
     "iopub.status.busy": "2024-06-26T14:24:30.313427Z",
     "iopub.status.idle": "2024-06-26T14:24:34.244216Z",
     "shell.execute_reply": "2024-06-26T14:24:34.242856Z",
     "shell.execute_reply.started": "2024-06-26T14:24:30.313701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils.logging:Running InferenceRecipe with resolved config:\n",
      "\n",
      "checkpointer:\n",
      "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/jovyan/content/mistral-7B\n",
      "  checkpoint_files:\n",
      "  - pytorch_model-00001-of-00002.bin\n",
      "  - pytorch_model-00002-of-00002.bin\n",
      "  model_type: MISTRAL\n",
      "  output_dir: /home/jovyan/content/mistral-7B\n",
      "  recipe_checkpoint: null\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "max_new_tokens: 300\n",
      "model:\n",
      "  _component_: torchtune.models.mistral.mistral_7b\n",
      "prompt: You are a powerful text-to-SQL model. Your job is to answer questions about\n",
      "  a database. You are given a question and context regarding one or more tables.\\n\\nYou\n",
      "  must output the SQL query that answers the question.\\n### Input:\\nWhich Class has\n",
      "  a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\\n\\n###\n",
      "  Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license\n",
      "  VARCHAR)\\n\\n### Response:\\n\n",
      "quantizer: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 1234\n",
      "temperature: 0.6\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.mistral.mistral_tokenizer\n",
      "  path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
      "top_k: 300\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/tune\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/tune.py\", line 49, in main\n",
      "    parser.run(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/tune.py\", line 43, in run\n",
      "    args.func(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/run.py\", line 179, in _run_cmd\n",
      "    self._run_single_device(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/run.py\", line 93, in _run_single_device\n",
      "    runpy.run_path(str(args.recipe), run_name=\"__main__\")\n",
      "  File \"<frozen runpy>\", line 291, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/recipes/generate.py\", line 152, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/config/_parse.py\", line 50, in wrapper\n",
      "    sys.exit(recipe_main(conf))\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/recipes/generate.py\", line 146, in main\n",
      "    recipe = InferenceRecipe(cfg=cfg)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/recipes/generate.py\", line 38, in __init__\n",
      "    self._dtype = utils.get_dtype(dtype=cfg.dtype)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/utils/precision.py\", line 122, in get_dtype\n",
      "    raise RuntimeError(\n",
      "RuntimeError: bf16 precision was requested but not available on this hardware. Please use fp32 precision instead.\n"
     ]
    }
   ],
   "source": [
    "! tune run generate --config ./mistral_generation.yaml \\\n",
    "prompt=\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\nYou must output the SQL query that answers the question.\\n### Input:\\nWhich Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\\n\\n### Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\\n\\n### Response:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dyamically get the response diplayed here and maybe lose the output above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, using the base model, the answer is not exactly what we were hoping for. This is because the objective of the model is next word prediction.\n",
    "\n",
    "To improve the model's understanding, we will fine tune it using the Alpaca dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we need to create a configuration file that holds the relevant information to fine tune the model.\n",
    "\n",
    "LoRA (Low-Rank Adaptation), a highly efficient method of LLM fine tuning, is here used via this configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T14:24:34.247399Z",
     "iopub.status.busy": "2024-06-26T14:24:34.246912Z",
     "iopub.status.idle": "2024-06-26T14:24:34.256354Z",
     "shell.execute_reply": "2024-06-26T14:24:34.255072Z",
     "shell.execute_reply.started": "2024-06-26T14:24:34.247346Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7B_lora_single_device_mistral.yaml\n",
    "\n",
    "with open(\"7B_lora_single_device_mistral.yaml\", \"w\") as fp:\n",
    "    fp.write(\n",
    "        \"\"\"\n",
    "        # Tokenizer\n",
    "        tokenizer:\n",
    "          _component_: torchtune.models.mistral.mistral_tokenizer\n",
    "          path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
    "        \n",
    "        # Dataset\n",
    "        dataset:\n",
    "          _component_: torchtune.datasets.alpaca_dataset\n",
    "          train_on_input: True\n",
    "        seed: null\n",
    "        shuffle: True\n",
    "        \n",
    "        # Model Arguments\n",
    "        model:\n",
    "          _component_: torchtune.models.mistral.lora_mistral_7b\n",
    "          lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']\n",
    "          apply_lora_to_mlp: True\n",
    "          apply_lora_to_output: True\n",
    "          lora_rank: 64\n",
    "          lora_alpha: 16\n",
    "        \n",
    "        checkpointer:\n",
    "          _component_: torchtune.utils.FullModelHFCheckpointer\n",
    "          checkpoint_dir: /home/jovyan/content/mistral-7B\n",
    "          checkpoint_files: [\n",
    "            pytorch_model-00001-of-00002.bin,\n",
    "            pytorch_model-00002-of-00002.bin\n",
    "          ]\n",
    "          recipe_checkpoint: null\n",
    "          output_dir: /home/jovyan/content/mistral-7B\n",
    "          model_type: MISTRAL\n",
    "        resume_from_checkpoint: False\n",
    "        \n",
    "        optimizer:\n",
    "          _component_: torch.optim.AdamW\n",
    "          lr: 2e-5\n",
    "        \n",
    "        lr_scheduler:\n",
    "          _component_: torchtune.modules.get_cosine_schedule_with_warmup\n",
    "          num_warmup_steps: 100\n",
    "        \n",
    "        loss:\n",
    "          _component_: torch.nn.CrossEntropyLoss\n",
    "        \n",
    "        # Fine-tuning arguments\n",
    "        batch_size: 4\n",
    "        epochs: 3\n",
    "        max_steps_per_epoch: null\n",
    "        gradient_accumulation_steps: 4\n",
    "        compile: False\n",
    "        \n",
    "        # Training env\n",
    "        device: cuda\n",
    "        \n",
    "        # Memory management\n",
    "        enable_activation_checkpointing: True\n",
    "        \n",
    "        # Reduced precision\n",
    "        dtype: bf16\n",
    "        \n",
    "        # Logging\n",
    "        metric_logger:\n",
    "          _component_: torchtune.utils.metric_logging.DiskLogger\n",
    "          log_dir: ${output_dir}\n",
    "        output_dir: /home/jovyan/content/mistral-7B\n",
    "        log_every_n_steps: null\n",
    "        \n",
    "        # Show case the usage of pytorch profiler\n",
    "        # Set enabled to False as it's only needed for debugging training\n",
    "        profiler:\n",
    "          _component_: torchtune.utils.profiler\n",
    "          enabled: False\n",
    "          output_dir: /home/jovyan/content/mistral-7B/torchtune_perf_tracing.json\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T14:24:34.258617Z",
     "iopub.status.busy": "2024-06-26T14:24:34.258172Z",
     "iopub.status.idle": "2024-06-26T14:24:38.101149Z",
     "shell.execute_reply": "2024-06-26T14:24:38.099587Z",
     "shell.execute_reply.started": "2024-06-26T14:24:34.258577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils.logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/jovyan/content/mistral-7B\n",
      "  checkpoint_files:\n",
      "  - pytorch_model-00001-of-00002.bin\n",
      "  - pytorch_model-00002-of-00002.bin\n",
      "  model_type: MISTRAL\n",
      "  output_dir: /home/jovyan/content/mistral-7B\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "  train_on_input: true\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "epochs: 3\n",
      "gradient_accumulation_steps: 4\n",
      "log_every_n_steps: null\n",
      "loss:\n",
      "  _component_: torch.nn.CrossEntropyLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.modules.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 100\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.utils.metric_logging.DiskLogger\n",
      "  log_dir: /home/jovyan/content/mistral-7B\n",
      "model:\n",
      "  _component_: torchtune.models.mistral.lora_mistral_7b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: true\n",
      "  lora_alpha: 16\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  lora_rank: 64\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  lr: 2.0e-05\n",
      "output_dir: /home/jovyan/content/mistral-7B\n",
      "profiler:\n",
      "  _component_: torchtune.utils.profiler\n",
      "  enabled: false\n",
      "  output_dir: /home/jovyan/content/mistral-7B/torchtune_perf_tracing.json\n",
      "resume_from_checkpoint: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.mistral.mistral_tokenizer\n",
      "  path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/tune\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/tune.py\", line 49, in main\n",
      "    parser.run(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/tune.py\", line 43, in run\n",
      "    args.func(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/run.py\", line 179, in _run_cmd\n",
      "    self._run_single_device(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/run.py\", line 93, in _run_single_device\n",
      "    runpy.run_path(str(args.recipe), run_name=\"__main__\")\n",
      "  File \"<frozen runpy>\", line 291, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/recipes/lora_finetune_single_device.py\", line 510, in <module>\n",
      "    sys.exit(recipe_main())\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/config/_parse.py\", line 50, in wrapper\n",
      "    sys.exit(recipe_main(conf))\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/recipes/lora_finetune_single_device.py\", line 503, in recipe_main\n",
      "    recipe = LoRAFinetuneRecipeSingleDevice(cfg=cfg)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/recipes/lora_finetune_single_device.py\", line 99, in __init__\n",
      "    self._dtype = utils.get_dtype(cfg.dtype, device=self._device)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/utils/precision.py\", line 122, in get_dtype\n",
      "    raise RuntimeError(\n",
      "RuntimeError: bf16 precision was requested but not available on this hardware. Please use fp32 precision instead.\n"
     ]
    }
   ],
   "source": [
    "! tune run lora_finetune_single_device --config ./7B_lora_single_device_mistral.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a NVIDIA H100 PCIe GPU, one epoch is completed in roughly 58 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using Mistral 7B fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, there is the configuration file that will be used to infere with the fine tuned version of the Mistral 7B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T14:24:38.104484Z",
     "iopub.status.busy": "2024-06-26T14:24:38.103354Z",
     "iopub.status.idle": "2024-06-26T14:24:38.112221Z",
     "shell.execute_reply": "2024-06-26T14:24:38.111219Z",
     "shell.execute_reply.started": "2024-06-26T14:24:38.104421Z"
    }
   },
   "outputs": [],
   "source": [
    "# mistral_fine-tuned_generation.yaml\n",
    "\n",
    "with open(\"mistral_fine-tuned_generation.yaml\", \"w\") as fp:\n",
    "    fp.write(\n",
    "        \"\"\"\n",
    "        # Config for running the InferenceRecipe in generate.py to generate output from an LLM\n",
    "        #\n",
    "        # To launch, run the following command from root torchtune directory:\n",
    "        #    tune run generate --config generation\n",
    "        \n",
    "        # Model arguments\n",
    "        model:\n",
    "          _component_: torchtune.models.mistral.mistral_7b\n",
    "        \n",
    "        checkpointer:\n",
    "          _component_: torchtune.utils.FullModelHFCheckpointer\n",
    "          checkpoint_dir: /home/jovyan/content/mistral-7B\n",
    "          checkpoint_files: [\n",
    "            hf_model_0001_2.pt,\n",
    "            hf_model_0002_2.pt,\n",
    "          ]\n",
    "          recipe_checkpoint: null\n",
    "          output_dir: /home/jovyan/content/mistral-7B\n",
    "          model_type: MISTRAL\n",
    "        resume_from_checkpoint: False\n",
    "        \n",
    "        device: cuda\n",
    "        dtype: bf16\n",
    "        \n",
    "        seed: 1234\n",
    "        \n",
    "        # Tokenizer arguments\n",
    "        tokenizer:\n",
    "          _component_: torchtune.models.mistral.mistral_tokenizer\n",
    "          path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
    "        \n",
    "        # Generation arguments; defaults taken from gpt-fast\n",
    "        prompt: \"Hello, my name is\"\n",
    "        max_new_tokens: 300\n",
    "        temperature: 0.6 # 0.8 and 0.6 are popular values to try\n",
    "        top_k: 300\n",
    "\n",
    "        quantizer: null\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the exact same prompt as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T14:24:38.114493Z",
     "iopub.status.busy": "2024-06-26T14:24:38.113679Z",
     "iopub.status.idle": "2024-06-26T14:24:41.935999Z",
     "shell.execute_reply": "2024-06-26T14:24:41.934265Z",
     "shell.execute_reply.started": "2024-06-26T14:24:38.114449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils.logging:Running InferenceRecipe with resolved config:\n",
      "\n",
      "checkpointer:\n",
      "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/jovyan/content/mistral-7B\n",
      "  checkpoint_files:\n",
      "  - hf_model_0001_2.pt\n",
      "  - hf_model_0002_2.pt\n",
      "  model_type: MISTRAL\n",
      "  output_dir: /home/jovyan/content/mistral-7B\n",
      "  recipe_checkpoint: null\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "max_new_tokens: 300\n",
      "model:\n",
      "  _component_: torchtune.models.mistral.mistral_7b\n",
      "prompt: You are a powerful text-to-SQL model. Your job is to answer questions about\n",
      "  a database. You are given a question and context regarding one or more tables.\\n\\nYou\n",
      "  must output the SQL query that answers the question.\\n### Input:\\nWhich Class has\n",
      "  a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\\n\\n###\n",
      "  Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license\n",
      "  VARCHAR)\\n\\n### Response:\\n\n",
      "quantizer: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 1234\n",
      "temperature: 0.6\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.mistral.mistral_tokenizer\n",
      "  path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
      "top_k: 300\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/tune\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/tune.py\", line 49, in main\n",
      "    parser.run(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/tune.py\", line 43, in run\n",
      "    args.func(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/run.py\", line 179, in _run_cmd\n",
      "    self._run_single_device(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/_cli/run.py\", line 93, in _run_single_device\n",
      "    runpy.run_path(str(args.recipe), run_name=\"__main__\")\n",
      "  File \"<frozen runpy>\", line 291, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/recipes/generate.py\", line 152, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/config/_parse.py\", line 50, in wrapper\n",
      "    sys.exit(recipe_main(conf))\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/recipes/generate.py\", line 146, in main\n",
      "    recipe = InferenceRecipe(cfg=cfg)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/recipes/generate.py\", line 38, in __init__\n",
      "    self._dtype = utils.get_dtype(dtype=cfg.dtype)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchtune/utils/precision.py\", line 122, in get_dtype\n",
      "    raise RuntimeError(\n",
      "RuntimeError: bf16 precision was requested but not available on this hardware. Please use fp32 precision instead.\n"
     ]
    }
   ],
   "source": [
    "! tune run generate --config ./mistral_fine-tuned_generation.yaml \\\n",
    "prompt=\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\nYou must output the SQL query that answers the question.\\n### Input:\\nWhich Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\\n\\n### Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\\n\\n### Response:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the output is more relevant, the model only outputs the answer to our question. The fine tuning process has worked!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
