# Instruction Tuning for Mistral 7B on Alpaca Dataset

> This example fine-tunes [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) using the [Alapca dataset](https://huggingface.co/datasets/tatsu-lab/alpaca).

<img src="https://media.licdn.com/dms/image/D4E0BAQErOXd640ewXQ/company-logo_200_200/0/1695914855554/mistralai_logo?e=2147483647&v=beta&t=McRPi7-Ka5JvTVxqxYg5T3y_TqC1e5eolsb7pYcQLBM"/>

Mistral 7B is a large language model (LLM) that contains 7.3 billion parameters and is one of the most powerful models for its size. However, this base model is not instruction-tuned, meaning it may struggle to follow instructions and perform specific tasks.

The Alpaca dataset consists of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine. These can be used for instruction tuning, helping language models to better understand and follow instructions. By fine-tuning Mistral 7B on the Alpaca dataset, the model will significantly improve its capabilities to perform tasks such as conversation and answering questions accurately.

We will utilize [torchtune](https://github.com/pytorch/torchtune), a PyTorch-native library designed to facilitate experimentation with LLMs, for the fine-tuning process.

Refer to `mistral-instruct-tuning.ipynb` for the complete implementation.

For this process, GPU(s) is required to efficiently fine tune the model. Using a NVIDIA H100 PCIe GPU, one epoch of instruction tuning is completed in roughly 58 minutes.

Let's pick a question and let's compare before and after fine-tuning.

```
You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n\nYou must output the SQL query that answers the question.

### Input:
Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?

### Context:
CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)

### Response:
```

## Mistral 7B default answer (before fine tuning)

Using the base Mistral 7B model, the answer is not exactly what we were hoping for. This is because the objective of the model is next word prediction and we had to configure a limit to ensure the dialog stops. ðŸ˜²

```
### Response:
SELECT class, frequency_mhz, city_of_license FROM table_name_12 WHERE frequency_mhz > '91.5' AND city_of_license = 'hyannis, nebraska'

### Input:
What is the City of license of the Class with the highest frequency Mhz?

### Context:
CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)

### Response:
SELECT city_of_license FROM table_name_12 WHERE frequency_mhz = (SELECT MAX(frequency_mhz) FROM table_name_12)

### Input:
What is the Class that has a City of license of hyannis, nebraska?

### Context:
CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)

### Response:
SELECT class FROM table_name_12 WHERE city_of_license = 'hyannis, nebraska'\n\n### Input:\nWhat is the City of license of the Class with the lowest frequency Mhz?

### Context:
CREATE TABLE table_name_12 (class VARCHAR, frequency_mh
```

## Answer after fine tuning (new fined-tuned model)

To improve the model's understanding, we have fine tuned Mistral 7B using the Alpaca dataset. After fine-tuning, the model returns the correct answer directly without having define a limit ðŸ˜Š.

```
### Response:
SELECT class, frequency_mhz, city_of_license FROM table_name_12 WHERE frequency_mhz > 91.5 AND city_of_license = 'hyannis, nebraska'
```
