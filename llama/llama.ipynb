{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the original source code available at [Modal Labs GitHub](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/openllama.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\echarles\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import GenerationConfig\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"openlm-research/open_llama_7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_models():\n",
    "    LlamaForCausalLM.from_pretrained(BASE_MODEL)\n",
    "    LlamaTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 180355072 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\echarles\\OneDrive - Business & Decision Benelux S.A\\Documents\\examples\\llama\\llama.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m download_models()\n",
      "\u001b[1;32mc:\\Users\\echarles\\OneDrive - Business & Decision Benelux S.A\\Documents\\examples\\llama\\llama.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_models\u001b[39m():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     LlamaForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(BASE_MODEL)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     LlamaTokenizer\u001b[39m.\u001b[39mfrom_pretrained(BASE_MODEL)\n",
      "File \u001b[1;32mc:\\Users\\echarles\\.conda\\envs\\openllama\\lib\\site-packages\\transformers\\modeling_utils.py:2629\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2626\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2628\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2629\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(config, \u001b[39m*\u001b[39mmodel_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2631\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[0;32m   2632\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[1;32mc:\\Users\\echarles\\.conda\\envs\\openllama\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:614\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[0;32m    613\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[1;32m--> 614\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m LlamaModel(config)\n\u001b[0;32m    616\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mhidden_size, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    618\u001b[0m     \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\echarles\\.conda\\envs\\openllama\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:445\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[0;32m    444\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[1;32m--> 445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    446\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\echarles\\.conda\\envs\\openllama\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:445\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[0;32m    444\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[1;32m--> 445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    446\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\echarles\\.conda\\envs\\openllama\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:256\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mhidden_size\n\u001b[0;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m LlamaAttention(config\u001b[39m=\u001b[39mconfig)\n\u001b[1;32m--> 256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m LlamaMLP(\n\u001b[0;32m    257\u001b[0m     hidden_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size,\n\u001b[0;32m    258\u001b[0m     intermediate_size\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mintermediate_size,\n\u001b[0;32m    259\u001b[0m     hidden_act\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mhidden_act,\n\u001b[0;32m    260\u001b[0m )\n\u001b[0;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[1;32mc:\\Users\\echarles\\.conda\\envs\\openllama\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:152\u001b[0m, in \u001b[0;36mLlamaMLP.__init__\u001b[1;34m(self, hidden_size, intermediate_size, hidden_act)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(hidden_size, intermediate_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(intermediate_size, hidden_size, bias\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(hidden_size, intermediate_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn \u001b[39m=\u001b[39m ACT2FN[hidden_act]\n",
      "File \u001b[1;32mc:\\Users\\echarles\\.conda\\envs\\openllama\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[1;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((out_features, in_features), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 180355072 bytes."
     ]
    }
   ],
   "source": [
    "download_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenLlamaModel:\n",
    "    def __enter__(self):\n",
    "\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "        self.tokenizer.bos_token_id = 1\n",
    "\n",
    "        model.eval()\n",
    "        self.model = torch.compile(model)\n",
    "        self.device = \"cuda\"\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input,\n",
    "        max_new_tokens=128,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        inputs = self.tokenizer(input, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "\n",
    "        generation_config = GenerationConfig(**kwargs)\n",
    "        with torch.no_grad():\n",
    "            generation_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        output = self.tokenizer.decode(s)\n",
    "        print(f\"\\033[96m{input}\\033[0m\")\n",
    "        print(output.split(input)[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    inputs = [\n",
    "        \"Building a website can be done in 10 simple steps:\",\n",
    "    ]\n",
    "    model = OpenLlamaModel()\n",
    "    for input in inputs:\n",
    "        model.generate(\n",
    "            input,\n",
    "            top_p=0.75,\n",
    "            top_k=40,\n",
    "            num_beams=1,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenLlamaModel' object has no attribute 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\echarles\\OneDrive - Business & Decision Benelux S.A\\Documents\\examples\\llama\\llama.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main()\n",
      "\u001b[1;32mc:\\Users\\echarles\\OneDrive - Business & Decision Benelux S.A\\Documents\\examples\\llama\\llama.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m OpenLlamaModel()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39min\u001b[39;00m inputs:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         top_p\u001b[39m=\u001b[39;49m\u001b[39m0.75\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         top_k\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         num_beams\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\echarles\\OneDrive - Business & Decision Benelux S.A\\Documents\\examples\\llama\\llama.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m ):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(\u001b[39minput\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     input_ids \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echarles/OneDrive%20-%20Business%20%26%20Decision%20Benelux%20S.A/Documents/examples/llama/llama.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     generation_config \u001b[39m=\u001b[39m GenerationConfig(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OpenLlamaModel' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
