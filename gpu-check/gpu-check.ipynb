{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568247b9-5152-404e-978b-b244e072a55b",
   "metadata": {},
   "source": [
    "# GPU sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626f648e-0ec3-4195-a5ab-616c7708f344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T16:59:11.776294Z",
     "iopub.status.busy": "2025-02-27T16:59:11.776169Z",
     "iopub.status.idle": "2025-02-27T16:59:13.134717Z",
     "shell.execute_reply": "2025-02-27T16:59:13.134114Z",
     "shell.execute_reply.started": "2025-02-27T16:59:11.776281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 27 16:59:12 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000001:00:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             46W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0456785a-a2f9-4478-a5f2-03360d2fa0ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T16:59:13.671616Z",
     "iopub.status.busy": "2025-02-27T16:59:13.671402Z",
     "iopub.status.idle": "2025-02-27T16:59:14.645772Z",
     "shell.execute_reply": "2025-02-27T16:59:14.645252Z",
     "shell.execute_reply.started": "2025-02-27T16:59:13.671592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Wed_Jan_15_19:20:09_PST_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.61\n",
      "Build cuda_12.8.r12.8/compiler.35404655_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d9d3a6e-61de-4b50-a5d8-1189042b4014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:03:01.926131Z",
     "iopub.status.busy": "2025-02-27T17:03:01.925916Z",
     "iopub.status.idle": "2025-02-27T17:03:02.844962Z",
     "shell.execute_reply": "2025-02-27T17:03:02.844440Z",
     "shell.execute_reply.started": "2025-02-27T17:03:01.926115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter-01jn1g9v33epps6f5f49rzeeyp\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6d0b04e-35a8-4b96-aa40-2fbb1d5310b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:03:47.443031Z",
     "iopub.status.busy": "2025-02-27T17:03:47.442765Z",
     "iopub.status.idle": "2025-02-27T17:03:49.001659Z",
     "shell.execute_reply": "2025-02-27T17:03:49.001044Z",
     "shell.execute_reply.started": "2025-02-27T17:03:47.443010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vector addition of 50000 elements]\n",
      "Copy input data from the host memory to the CUDA device\n",
      "CUDA kernel launch with 196 blocks of 256 threads\n",
      "Copy output data from the CUDA device to the host memory\n",
      "Test PASSED\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!/tmp/cuda-samples/Samples/0_Introduction/vectorAdd/vectorAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1a7e4-b93a-41a9-b470-4e952ad28d30",
   "metadata": {},
   "source": [
    "## Checking GPU availability and memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "321ea026-5d6b-42bd-b050-9b7e885493a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:03:58.976473Z",
     "iopub.status.busy": "2025-02-27T17:03:58.976171Z",
     "iopub.status.idle": "2025-02-27T17:04:00.316028Z",
     "shell.execute_reply": "2025-02-27T17:04:00.315438Z",
     "shell.execute_reply.started": "2025-02-27T17:03:58.976457Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b7390c-c993-41f7-97cd-8081fc1a7e54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:04:00.317121Z",
     "iopub.status.busy": "2025-02-27T17:04:00.316878Z",
     "iopub.status.idle": "2025-02-27T17:04:00.484319Z",
     "shell.execute_reply": "2025-02-27T17:04:00.483676Z",
     "shell.execute_reply.started": "2025-02-27T17:04:00.317107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x7fb34433c950>\n",
      "NVIDIA A100 80GB PCIe\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "233b9170-dfd6-44f7-8c47-8ec744f1737e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:04:05.785645Z",
     "iopub.status.busy": "2025-02-27T17:04:05.785400Z",
     "iopub.status.idle": "2025-02-27T17:04:05.790207Z",
     "shell.execute_reply": "2025-02-27T17:04:05.789542Z",
     "shell.execute_reply.started": "2025-02-27T17:04:05.785629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84974239744\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t)\n",
    "print(r)\n",
    "print(a)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "817e5840-6cd4-49f0-bc13-17b385dc0f63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:05:41.228322Z",
     "iopub.status.busy": "2025-02-27T17:05:41.228091Z",
     "iopub.status.idle": "2025-02-27T17:05:41.255202Z",
     "shell.execute_reply": "2025-02-27T17:05:41.254588Z",
     "shell.execute_reply.started": "2025-02-27T17:05:41.228306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bbb207c-41bc-4fee-b8d8-2e6fd702911f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:04:08.468713Z",
     "iopub.status.busy": "2025-02-27T17:04:08.468464Z",
     "iopub.status.idle": "2025-02-27T17:04:08.473018Z",
     "shell.execute_reply": "2025-02-27T17:04:08.472374Z",
     "shell.execute_reply.started": "2025-02-27T17:04:08.468686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226764500\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "with open('/proc/meminfo') as f:\n",
    "    meminfo = f.read()\n",
    "matched = re.search(r'^MemTotal:\\s+(\\d+)', meminfo)\n",
    "if matched: \n",
    "    mem_total_kB = int(matched.groups()[0])\n",
    "print(mem_total_kB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1fc9a72-b45a-4b47-ad41-2f270e445cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:04:11.074527Z",
     "iopub.status.busy": "2025-02-27T17:04:11.074280Z",
     "iopub.status.idle": "2025-02-27T17:04:11.078267Z",
     "shell.execute_reply": "2025-02-27T17:04:11.077632Z",
     "shell.execute_reply.started": "2025-02-27T17:04:11.074507Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to check the number of GPUs and their memory\n",
    "def print_gpus():\n",
    "    gpu_count = cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {gpu_count}\")\n",
    "    for i in range(gpu_count):\n",
    "        gpu_properties = cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {gpu_properties.name}\")\n",
    "        print(f\"  Total Memory: {gpu_properties.total_memory / 1024 ** 3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11d551eb-6e4c-469d-9f7c-200c6ef4513e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:04:11.508599Z",
     "iopub.status.busy": "2025-02-27T17:04:11.508418Z",
     "iopub.status.idle": "2025-02-27T17:04:11.512384Z",
     "shell.execute_reply": "2025-02-27T17:04:11.511638Z",
     "shell.execute_reply.started": "2025-02-27T17:04:11.508584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "  Total Memory: 79.14 GB\n"
     ]
    }
   ],
   "source": [
    "print_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "deb534c1-0c0c-4abd-9d28-7ff3c7428ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:08:49.313205Z",
     "iopub.status.busy": "2025-02-27T17:08:49.312967Z",
     "iopub.status.idle": "2025-02-27T17:08:49.316898Z",
     "shell.execute_reply": "2025-02-27T17:08:49.316423Z",
     "shell.execute_reply.started": "2025-02-27T17:08:49.313189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "  Total Memory: 79.14 GB\n"
     ]
    }
   ],
   "source": [
    "# To check the number of GPUs and their memory\n",
    "\n",
    "gpu_count = cuda.device_count()\n",
    "print(f\"Number of available GPUs: {gpu_count}\")\n",
    "\n",
    "for i in range(gpu_count):\n",
    "    gpu_properties = cuda.get_device_properties(i)\n",
    "    print(f\"GPU {i}: {gpu_properties.name}\")\n",
    "    print(f\"  Total Memory: {gpu_properties.total_memory / 1024 ** 3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20153dd-4908-42a2-ac90-c92572137bfa",
   "metadata": {},
   "source": [
    "## Validating expected GPU count and memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d617b4f0-d687-43ff-924a-ebb0a5829b45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:12:25.073701Z",
     "iopub.status.busy": "2025-02-27T17:12:25.073433Z",
     "iopub.status.idle": "2025-02-27T17:12:25.076528Z",
     "shell.execute_reply": "2025-02-27T17:12:25.076017Z",
     "shell.execute_reply.started": "2025-02-27T17:12:25.073680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "expected_gpu_count = 1  # Change as needed\n",
    "expected_memory_gb = 79  # Change as needed (per GPU)\n",
    "tensor_size_gb = 79  # Change as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69dbe939-bd0b-4be1-bf19-a387bfa02b49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:12:25.624324Z",
     "iopub.status.busy": "2025-02-27T17:12:25.624053Z",
     "iopub.status.idle": "2025-02-27T17:12:25.628288Z",
     "shell.execute_reply": "2025-02-27T17:12:25.627766Z",
     "shell.execute_reply.started": "2025-02-27T17:12:25.624302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 has sufficient memory: 79.14 GB\n"
     ]
    }
   ],
   "source": [
    "# To validate the expected number of GPUs and memory\n",
    "\n",
    "actual_gpu_count = cuda.device_count()\n",
    "\n",
    "def validate_gpus(expected_gpu_count, expected_memory_gb):\n",
    "    \n",
    "    if actual_gpu_count < expected_gpu_count:\n",
    "        raise ValueError(f\"Expected at least {expected_gpu_count} GPUs, but found {actual_gpu_count}\")\n",
    "\n",
    "    for i in range(expected_gpu_count):\n",
    "        gpu_properties = cuda.get_device_properties(i)\n",
    "        actual_memory_gb = gpu_properties.total_memory / 1024 ** 3\n",
    "        if actual_memory_gb < expected_memory_gb:\n",
    "            raise ValueError(f\"Expected GPU {i} to have at least {expected_memory_gb} GB, but found {actual_memory_gb:.2f} GB\")\n",
    "        print(f\"GPU {i} has sufficient memory: {actual_memory_gb:.2f} GB\")\n",
    "\n",
    "validate_gpus(expected_gpu_count, expected_memory_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d9a793b-b45b-4638-b29e-73c988e4d519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:12:26.158737Z",
     "iopub.status.busy": "2025-02-27T17:12:26.158510Z",
     "iopub.status.idle": "2025-02-27T17:12:27.933530Z",
     "shell.execute_reply": "2025-02-27T17:12:27.932995Z",
     "shell.execute_reply.started": "2025-02-27T17:12:26.158715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing tensor load on GPU 0...\n",
      "Failed to load tensor of size 79 GB onto GPU 0: CUDA out of memory. Tried to allocate 79.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 78.65 GiB is free. Process 957717 has 492.00 MiB memory in use. Of the allocated memory 17.50 KiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Testing oversized tensor load on GPU 0...\n",
      "Correctly failed to load tensor of size 158 GB onto GPU 0: CUDA out of memory. Tried to allocate 158.00 GiB. GPU 0 has a total capacity of 79.14 GiB of which 78.65 GiB is free. Process 957717 has 492.00 MiB memory in use. Of the allocated memory 17.50 KiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# Function to test loading a tensor onto the GPU\n",
    "def test_tensor_load(gpu_index, size_gb):\n",
    "    tensor_size = int(size_gb * 1024 ** 3 / 4)  # size in floats (4 bytes per float)\n",
    "    device = torch.device(f'cuda:{gpu_index}')\n",
    "    try:\n",
    "        tensor = torch.rand(tensor_size, device=device)\n",
    "        print(f\"Successfully loaded tensor of size {size_gb} GB onto GPU {gpu_index}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to load tensor of size {size_gb} GB onto GPU {gpu_index}: {e}\")\n",
    "        \n",
    "# Function to test loading an oversized tensor onto the GPU\n",
    "def test_oversized_tensor_load(gpu_index, size_gb):\n",
    "    tensor_size = int(size_gb * 1024 ** 3 / 4)  # size in floats (4 bytes per float)\n",
    "    device = torch.device(f'cuda:{gpu_index}')\n",
    "    try:\n",
    "        tensor = torch.rand(tensor_size, device=device)\n",
    "        print(f\"Unexpectedly succeeded in loading tensor of size {size_gb} GB onto GPU {gpu_index}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Correctly failed to load tensor of size {size_gb} GB onto GPU {gpu_index}: {e}\")\n",
    "    \n",
    "for i in range(expected_gpu_count):\n",
    "    print(f\"\\nTesting tensor load on GPU {i}...\")\n",
    "    test_tensor_load(i, tensor_size_gb)\n",
    "    print(f\"\\nTesting oversized tensor load on GPU {i}...\")\n",
    "    test_oversized_tensor_load(i, 2 * tensor_size_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d49e3-6e58-4a53-a3b4-7281b1ed73cd",
   "metadata": {},
   "source": [
    "The validation confirms that the current system configuration meets the expected requirements:\n",
    "- **GPU Count:** The expected number of GPUs is available.\n",
    "- **GPU Memory:** Each GPU has sufficient memory to support the planned workload.\n",
    "\n",
    "This ensures that the system is adequately provisioned for the tasks requiring GPU resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ea22b-85a4-4c7c-aa2e-292d852bd58c",
   "metadata": {},
   "source": [
    "## More Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55ddddcb-1431-4f65-848e-bb74d7bf4cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:12:34.320915Z",
     "iopub.status.busy": "2025-02-27T17:12:34.320688Z",
     "iopub.status.idle": "2025-02-27T17:12:34.324478Z",
     "shell.execute_reply": "2025-02-27T17:12:34.324014Z",
     "shell.execute_reply.started": "2025-02-27T17:12:34.320899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.FloatTensor([0., 1., 2.])\n",
    "print(X_train.is_cuda)\n",
    "X_train = X_train.to(device)\n",
    "print(X_train.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e38df918-d422-4564-b594-f11e50e5d151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:12:35.217560Z",
     "iopub.status.busy": "2025-02-27T17:12:35.217374Z",
     "iopub.status.idle": "2025-02-27T17:12:35.474601Z",
     "shell.execute_reply": "2025-02-27T17:12:35.474168Z",
     "shell.execute_reply.started": "2025-02-27T17:12:35.217545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 754.3153076171875\n",
      "199 754.3112182617188\n",
      "299 754.3071899414062\n",
      "399 754.3031005859375\n",
      "499 754.2990112304688\n",
      "599 754.2949829101562\n",
      "699 754.2909545898438\n",
      "799 754.2869262695312\n",
      "899 754.2828979492188\n",
      "999 754.27880859375\n",
      "1099 754.2747802734375\n",
      "1199 754.270751953125\n",
      "1299 754.2667236328125\n",
      "1399 754.2626953125\n",
      "1499 754.2586669921875\n",
      "Result: y = -0.37436068058013916 + -0.8177333474159241 x + 0.864264190196991 x^2 + 0.384154349565506 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "# device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") \n",
    "data_type = torch.float\n",
    "x = torch.linspace(-math.pi, math.pi, 1500, device=device, dtype=data_type)\n",
    "y = torch.sin(x)\n",
    "a = torch.randn((), device=device, dtype=data_type)\n",
    "b = torch.randn((), device=device, dtype=data_type)\n",
    "c = torch.randn((), device=device, dtype=data_type)\n",
    "d = torch.randn((), device=device, dtype=data_type)\n",
    "learning_rate = 1e-6\n",
    "m = 1\n",
    "for i in range(1500):\n",
    "    y_pred = a + b * m + c * m ** 2 + d * m ** 3\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if i % 100 == 99:\n",
    "        print(i, loss)\n",
    "    grad_a = y_pred.sum()\n",
    "    grad_b = (y_pred * m).sum()\n",
    "    grad_c = (y_pred * m** 2).sum()\n",
    "    grad_d = (y_pred * m ** 3).sum()\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bfa84ff-bf7d-4b3b-9f8f-8eac53addbff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-27T17:14:19.951513Z",
     "iopub.status.busy": "2025-02-27T17:14:19.951257Z",
     "iopub.status.idle": "2025-02-27T17:14:19.994174Z",
     "shell.execute_reply": "2025-02-27T17:14:19.993668Z",
     "shell.execute_reply.started": "2025-02-27T17:14:19.951496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9a95c94eee431a9d6a40b65b96659c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider\n",
    "IntSlider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4811383-ef2c-4884-90ca-4e6b03b3f0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
