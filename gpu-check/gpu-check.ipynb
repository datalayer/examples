{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568247b9-5152-404e-978b-b244e072a55b",
   "metadata": {},
   "source": [
    "# GPU sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626f648e-0ec3-4195-a5ab-616c7708f344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T13:17:24.301536Z",
     "iopub.status.busy": "2024-07-24T13:17:24.300998Z",
     "iopub.status.idle": "2024-07-24T13:17:24.416798Z",
     "shell.execute_reply": "2024-07-24T13:17:24.416234Z",
     "shell.execute_reply.started": "2024-07-24T13:17:24.301517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d94c9-d7e9-460d-b4c6-dae41cf60745",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ab90b-8774-48b5-ad2c-15cf79506cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af73b6-ddf9-4f47-af4a-7e91768b03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./.cuda-samples/Samples/0_Introduction/vectorAdd/vectorAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785623ef-3da3-4241-b2c0-9566e1dcf742",
   "metadata": {},
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e047c85a-ba21-4e70-8cd5-ccde10130161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T11:13:53.051941Z",
     "iopub.status.busy": "2024-06-27T11:13:53.050643Z",
     "iopub.status.idle": "2024-06-27T11:13:53.058053Z",
     "shell.execute_reply": "2024-06-27T11:13:53.056723Z",
     "shell.execute_reply.started": "2024-06-27T11:13:53.051882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.cuda as cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7390c-c993-41f7-97cd-8081fc1a7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651320c1-faca-4145-b8d4-6ae660119b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b9170-dfd6-44f7-8c47-8ec744f1737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t)\n",
    "print(r)\n",
    "print(a)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb207c-41bc-4fee-b8d8-2e6fd702911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('/proc/meminfo') as f:\n",
    "    meminfo = f.read()\n",
    "matched = re.search(r'^MemTotal:\\s+(\\d+)', meminfo)\n",
    "if matched: \n",
    "    mem_total_kB = int(matched.groups()[0])\n",
    "print(mem_total_kB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2641df6-7b72-4cda-87d2-28bee435a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "IntSlider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1fc9a72-b45a-4b47-ad41-2f270e445cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T11:13:53.741134Z",
     "iopub.status.busy": "2024-06-27T11:13:53.740026Z",
     "iopub.status.idle": "2024-06-27T11:13:53.755562Z",
     "shell.execute_reply": "2024-06-27T11:13:53.754252Z",
     "shell.execute_reply.started": "2024-06-27T11:13:53.741081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to check the number of GPUs and their memory\n",
    "def check_gpus():\n",
    "    gpu_count = cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {gpu_count}\")\n",
    "    for i in range(gpu_count):\n",
    "        gpu_properties = cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {gpu_properties.name}\")\n",
    "        print(f\"  Total Memory: {gpu_properties.total_memory / 1024 ** 3:.2f} GB\")\n",
    "\n",
    "# Function to validate the expected number of GPUs and memory\n",
    "def validate_gpus(expected_gpu_count, expected_memory_gb):\n",
    "    actual_gpu_count = cuda.device_count()\n",
    "    if actual_gpu_count < expected_gpu_count:\n",
    "        raise ValueError(f\"Expected at least {expected_gpu_count} GPUs, but found {actual_gpu_count}\")\n",
    "    \n",
    "    for i in range(expected_gpu_count):\n",
    "        gpu_properties = cuda.get_device_properties(i)\n",
    "        actual_memory_gb = gpu_properties.total_memory / 1024 ** 3\n",
    "        if actual_memory_gb < expected_memory_gb:\n",
    "            raise ValueError(f\"Expected GPU {i} to have at least {expected_memory_gb} GB, but found {actual_memory_gb:.2f} GB\")\n",
    "        print(f\"GPU {i} has sufficient memory: {actual_memory_gb:.2f} GB\")\n",
    "\n",
    "# Function to test loading a tensor onto the GPU\n",
    "def test_tensor_load(gpu_index, size_gb):\n",
    "    tensor_size = int(size_gb * 1024 ** 3 / 4)  # size in floats (4 bytes per float)\n",
    "    device = torch.device(f'cuda:{gpu_index}')\n",
    "    try:\n",
    "        tensor = torch.rand(tensor_size, device=device)\n",
    "        print(f\"Successfully loaded tensor of size {size_gb} GB onto GPU {gpu_index}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to load tensor of size {size_gb} GB onto GPU {gpu_index}: {e}\")\n",
    "\n",
    "# Function to test loading an oversized tensor onto the GPU\n",
    "def test_oversized_tensor_load(gpu_index, size_gb):\n",
    "    tensor_size = int(size_gb * 1024 ** 3 / 4)  # size in floats (4 bytes per float)\n",
    "    device = torch.device(f'cuda:{gpu_index}')\n",
    "    try:\n",
    "        tensor = torch.rand(tensor_size, device=device)\n",
    "        print(f\"Unexpectedly succeeded in loading tensor of size {size_gb} GB onto GPU {gpu_index}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Correctly failed to load tensor of size {size_gb} GB onto GPU {gpu_index}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1a7e4-b93a-41a9-b470-4e952ad28d30",
   "metadata": {},
   "source": [
    "## Checking GPU availability and memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d551eb-6e4c-469d-9f7c-200c6ef4513e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T11:13:55.525271Z",
     "iopub.status.busy": "2024-06-27T11:13:55.523994Z",
     "iopub.status.idle": "2024-06-27T11:13:55.532686Z",
     "shell.execute_reply": "2024-06-27T11:13:55.531242Z",
     "shell.execute_reply.started": "2024-06-27T11:13:55.525198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n",
      "GPU 0: Tesla V100S-PCIE-32GB\n",
      "  Total Memory: 31.73 GB\n",
      "GPU 1: Tesla V100S-PCIE-32GB\n",
      "  Total Memory: 31.73 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20153dd-4908-42a2-ac90-c92572137bfa",
   "metadata": {},
   "source": [
    "## Validating expected GPU count and memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8c7318-5862-42d2-ad18-c23e4b85a31b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T11:13:54.455113Z",
     "iopub.status.busy": "2024-06-27T11:13:54.453966Z",
     "iopub.status.idle": "2024-06-27T11:13:54.461919Z",
     "shell.execute_reply": "2024-06-27T11:13:54.460130Z",
     "shell.execute_reply.started": "2024-06-27T11:13:54.455063Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "expected_gpu_count = 2  # Change as needed\n",
    "expected_memory_gb = 31  # Change as needed (per GPU)\n",
    "tensor_size_gb = 31  # Change as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10fdea83-2b30-4223-8dfb-2fb94d0faad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T11:13:56.280912Z",
     "iopub.status.busy": "2024-06-27T11:13:56.279857Z",
     "iopub.status.idle": "2024-06-27T11:13:58.194581Z",
     "shell.execute_reply": "2024-06-27T11:13:58.193629Z",
     "shell.execute_reply.started": "2024-06-27T11:13:56.280862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 has sufficient memory: 31.73 GB\n",
      "GPU 1 has sufficient memory: 31.73 GB\n",
      "\n",
      "Testing tensor load on GPU 0...\n",
      "Successfully loaded tensor of size 31 GB onto GPU 0\n",
      "\n",
      "Testing oversized tensor load on GPU 0...\n",
      "Correctly failed to load tensor of size 62 GB onto GPU 0: CUDA out of memory. Tried to allocate 62.00 GiB. GPU 0 has a total capacity of 31.73 GiB of which 31.43 GiB is free. Process 837903 has 308.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "Testing tensor load on GPU 1...\n",
      "Successfully loaded tensor of size 31 GB onto GPU 1\n",
      "\n",
      "Testing oversized tensor load on GPU 1...\n",
      "Correctly failed to load tensor of size 62 GB onto GPU 1: CUDA out of memory. Tried to allocate 62.00 GiB. GPU 1 has a total capacity of 31.73 GiB of which 31.43 GiB is free. Process 837903 has 308.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "validate_gpus(expected_gpu_count, expected_memory_gb)\n",
    "\n",
    "for i in range(expected_gpu_count):\n",
    "    print(f\"\\nTesting tensor load on GPU {i}...\")\n",
    "    test_tensor_load(i, tensor_size_gb)\n",
    "    \n",
    "    print(f\"\\nTesting oversized tensor load on GPU {i}...\")\n",
    "    test_oversized_tensor_load(i, 2 * tensor_size_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d49e3-6e58-4a53-a3b4-7281b1ed73cd",
   "metadata": {},
   "source": [
    "The validation confirms that the current system configuration meets the expected requirements:\n",
    "- **GPU Count:** The expected number of GPUs (2) is available.\n",
    "- **GPU Memory:** Each GPU has sufficient memory (31 GB per GPU) to support the planned workload.\n",
    "This ensures that the system is adequately provisioned for the tasks requiring GPU resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ea22b-85a4-4c7c-aa2e-292d852bd58c",
   "metadata": {},
   "source": [
    "## More Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddddcb-1431-4f65-848e-bb74d7bf4cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor([0., 1., 2.])\n",
    "print(X_train.is_cuda)\n",
    "X_train = X_train.to(device)\n",
    "print(X_train.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38df918-d422-4564-b594-f11e50e5d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "# device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") \n",
    "data_type = torch.float\n",
    "x = torch.linspace(-math.pi, math.pi, 1500, device=device, dtype=data_type)\n",
    "y = torch.sin(x)\n",
    "a = torch.randn((), device=device, dtype=data_type)\n",
    "b = torch.randn((), device=device, dtype=data_type)\n",
    "c = torch.randn((), device=device, dtype=data_type)\n",
    "d = torch.randn((), device=device, dtype=data_type)\n",
    "learning_rate = 1e-6\n",
    "m = 1\n",
    "for i in range(1500):\n",
    "    y_pred = a + b * m + c * m ** 2 + d * m ** 3\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if i % 100 == 99:\n",
    "        print(i, loss)\n",
    "    grad_a = y_pred.sum()\n",
    "    grad_b = (y_pred * m).sum()\n",
    "    grad_c = (y_pred * m** 2).sum()\n",
    "    grad_d = (y_pred * m ** 3).sum()\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b475f-e270-4944-87a9-ee4a8af6b627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
