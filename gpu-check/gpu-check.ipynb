{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568247b9-5152-404e-978b-b244e072a55b",
   "metadata": {},
   "source": [
    "# GPU sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626f648e-0ec3-4195-a5ab-616c7708f344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:53:43.783147Z",
     "iopub.status.busy": "2024-08-23T11:53:43.781401Z",
     "iopub.status.idle": "2024-08-23T11:53:44.155183Z",
     "shell.execute_reply": "2024-08-23T11:53:44.153817Z",
     "shell.execute_reply.started": "2024-08-23T11:53:43.783077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 23 11:53:43 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100S-PCIE-32GB          On  |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   40C    P0             27W /  250W |       0MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1a7e4-b93a-41a9-b470-4e952ad28d30",
   "metadata": {},
   "source": [
    "## Checking GPU availability and memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321ea026-5d6b-42bd-b050-9b7e885493a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:53:44.159312Z",
     "iopub.status.busy": "2024-08-23T11:53:44.158188Z",
     "iopub.status.idle": "2024-08-23T11:53:45.560362Z",
     "shell.execute_reply": "2024-08-23T11:53:45.559828Z",
     "shell.execute_reply.started": "2024-08-23T11:53:44.159255Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb534c1-0c0c-4abd-9d28-7ff3c7428ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:53:45.561704Z",
     "iopub.status.busy": "2024-08-23T11:53:45.561106Z",
     "iopub.status.idle": "2024-08-23T11:53:45.608616Z",
     "shell.execute_reply": "2024-08-23T11:53:45.608135Z",
     "shell.execute_reply.started": "2024-08-23T11:53:45.561681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: Tesla V100S-PCIE-32GB\n",
      "  Total Memory: 31.73 GB\n"
     ]
    }
   ],
   "source": [
    "# To check the number of GPUs and their memory\n",
    "\n",
    "gpu_count = cuda.device_count()\n",
    "print(f\"Number of available GPUs: {gpu_count}\")\n",
    "for i in range(gpu_count):\n",
    "    gpu_properties = cuda.get_device_properties(i)\n",
    "    print(f\"GPU {i}: {gpu_properties.name}\")\n",
    "    print(f\"  Total Memory: {gpu_properties.total_memory / 1024 ** 3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d617b4f0-d687-43ff-924a-ebb0a5829b45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:53:45.609788Z",
     "iopub.status.busy": "2024-08-23T11:53:45.609326Z",
     "iopub.status.idle": "2024-08-23T11:53:45.612195Z",
     "shell.execute_reply": "2024-08-23T11:53:45.611814Z",
     "shell.execute_reply.started": "2024-08-23T11:53:45.609768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "expected_gpu_count = 1  # Change as needed\n",
    "expected_memory_gb = 31  # Change as needed (per GPU)\n",
    "tensor_size_gb = 31  # Change as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69dbe939-bd0b-4be1-bf19-a387bfa02b49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:53:45.613544Z",
     "iopub.status.busy": "2024-08-23T11:53:45.613080Z",
     "iopub.status.idle": "2024-08-23T11:53:45.617363Z",
     "shell.execute_reply": "2024-08-23T11:53:45.616841Z",
     "shell.execute_reply.started": "2024-08-23T11:53:45.613525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 has sufficient memory: 31.73 GB\n"
     ]
    }
   ],
   "source": [
    "# To validate the expected number of GPUs and memory\n",
    "\n",
    "actual_gpu_count = cuda.device_count()\n",
    "if actual_gpu_count < expected_gpu_count:\n",
    "    raise ValueError(f\"Expected at least {expected_gpu_count} GPUs, but found {actual_gpu_count}\")\n",
    "\n",
    "for i in range(expected_gpu_count):\n",
    "    gpu_properties = cuda.get_device_properties(i)\n",
    "    actual_memory_gb = gpu_properties.total_memory / 1024 ** 3\n",
    "    if actual_memory_gb < expected_memory_gb:\n",
    "        raise ValueError(f\"Expected GPU {i} to have at least {expected_memory_gb} GB, but found {actual_memory_gb:.2f} GB\")\n",
    "    print(f\"GPU {i} has sufficient memory: {actual_memory_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9a793b-b45b-4638-b29e-73c988e4d519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:53:45.618528Z",
     "iopub.status.busy": "2024-08-23T11:53:45.618103Z",
     "iopub.status.idle": "2024-08-23T11:53:46.685276Z",
     "shell.execute_reply": "2024-08-23T11:53:46.684791Z",
     "shell.execute_reply.started": "2024-08-23T11:53:45.618511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing tensor load on GPU 0...\n",
      "Successfully loaded tensor of size 31 GB onto GPU 0\n",
      "\n",
      "Testing oversized tensor load on GPU 0...\n",
      "Correctly failed to load tensor of size 62 GB onto GPU 0: CUDA out of memory. Tried to allocate 62.00 GiB. GPU \n"
     ]
    }
   ],
   "source": [
    "# Function to test loading a tensor onto the GPU\n",
    "def test_tensor_load(gpu_index, size_gb):\n",
    "    tensor_size = int(size_gb * 1024 ** 3 / 4)  # size in floats (4 bytes per float)\n",
    "    device = torch.device(f'cuda:{gpu_index}')\n",
    "    try:\n",
    "        tensor = torch.rand(tensor_size, device=device)\n",
    "        print(f\"Successfully loaded tensor of size {size_gb} GB onto GPU {gpu_index}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to load tensor of size {size_gb} GB onto GPU {gpu_index}: {e}\")\n",
    "        \n",
    "# Function to test loading an oversized tensor onto the GPU\n",
    "def test_oversized_tensor_load(gpu_index, size_gb):\n",
    "    tensor_size = int(size_gb * 1024 ** 3 / 4)  # size in floats (4 bytes per float)\n",
    "    device = torch.device(f'cuda:{gpu_index}')\n",
    "    try:\n",
    "        tensor = torch.rand(tensor_size, device=device)\n",
    "        print(f\"Unexpectedly succeeded in loading tensor of size {size_gb} GB onto GPU {gpu_index}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Correctly failed to load tensor of size {size_gb} GB onto GPU {gpu_index}: {e}\")\n",
    "    \n",
    "for i in range(expected_gpu_count):\n",
    "    print(f\"\\nTesting tensor load on GPU {i}...\")\n",
    "    test_tensor_load(i, tensor_size_gb)\n",
    "    \n",
    "    print(f\"\\nTesting oversized tensor load on GPU {i}...\")\n",
    "    test_oversized_tensor_load(i, 2 * tensor_size_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d49e3-6e58-4a53-a3b4-7281b1ed73cd",
   "metadata": {},
   "source": [
    "The validation confirms that the current system configuration meets the expected requirements:\n",
    "- **GPU Count:** The expected number of GPUs is available.\n",
    "- **GPU Memory:** Each GPU has sufficient memory to support the planned workload.\n",
    "\n",
    "This ensures that the system is adequately provisioned for the tasks requiring GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa84ff-bf7d-4b3b-9f8f-8eac53addbff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
