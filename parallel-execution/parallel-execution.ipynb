{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1b47b0-77d3-49f7-9e34-46f7425a0b5c",
   "metadata": {},
   "source": [
    "# Performance comparison of CPU and GPU serial and parallel execution\n",
    "\n",
    "This notebook explores the performance differences between serial and parallel execution on CPU and GPU using PyTorch. We'll compare the execution times of intensive computational tasks performed sequentially on CPU and GPU, as well as in parallel configurations.\n",
    "\n",
    "We will:\n",
    "- Define a function `intensive_computation` that performs a computationally intensive task using PyTorch operations.\n",
    "- Implement serial execution on CPU (`serial_cpu`) and GPU (`serial_gpu`).\n",
    "- Implement parallel execution on multiple CPU cores (`parallel_cpu`) using Python's `multiprocessing.Pool`.\n",
    "- Compare the execution times across these different configurations using a significant workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636069eb-f708-4f13-b166-1cd9e090b147",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-03-11T06:57:21.554280Z",
     "iopub.status.busy": "2025-03-11T06:57:21.554049Z",
     "iopub.status.idle": "2025-03-11T06:57:22.922579Z",
     "shell.execute_reply": "2025-03-11T06:57:22.921947Z",
     "shell.execute_reply.started": "2025-03-11T06:57:21.554262Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "\n",
    "# Function to perform a more intensive computation\n",
    "def intensive_computation(size):\n",
    "    result = 0\n",
    "    for i in range(size):\n",
    "        result += torch.sum(torch.rand(1000) ** 2)  # Squaring to make it more intensive\n",
    "\n",
    "# Serial execution on CPU\n",
    "def serial_cpu(size):\n",
    "    start_time = time.time()\n",
    "    intensive_computation(size)\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Serial CPU Execution Time: {duration:.4f} seconds\")\n",
    "\n",
    "# Serial execution on GPU (if available)\n",
    "def serial_gpu(size):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            intensive_computation(size)\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Serial GPU Execution Time: {duration:.4f} seconds\")\n",
    "    else:\n",
    "        print(\"No GPU available, cannot perform serial GPU execution.\")\n",
    "\n",
    "# Parallel execution on multiple CPU cores\n",
    "def parallel_cpu(size):\n",
    "    start_time = time.time()\n",
    "    processes = []\n",
    "    num_cpus = mp.cpu_count()\n",
    "    print(f\"Number of CPUs available: {num_cpus}\")\n",
    "    for _ in range(num_cpus):\n",
    "        p = mp.Process(target=intensive_computation, args=(size // num_cpus,))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Parallel CPU Execution Time: {duration:.4f} seconds\")\n",
    "\n",
    "# Parallel execution on multiple GPUs\n",
    "def parallel_gpu(size):\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        models = [nn.Sequential(nn.Linear(1000, 1000)).cuda() for _ in range(num_gpus)]\n",
    "\n",
    "        processes = []\n",
    "        for i in range(num_gpus):\n",
    "            p = mp.Process(target=intensive_computation, args=(size // num_gpus,))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Parallel GPU Execution Time: {duration:.4f} seconds\")\n",
    "    else:\n",
    "        print(\"No GPUs available, cannot perform parallel GPU execution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599f5882-78b6-47fb-9024-ed17e280ff07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T06:57:26.010738Z",
     "iopub.status.busy": "2025-03-11T06:57:26.010303Z",
     "iopub.status.idle": "2025-03-11T06:59:05.093209Z",
     "shell.execute_reply": "2025-03-11T06:59:05.091897Z",
     "shell.execute_reply.started": "2025-03-11T06:57:26.010720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Serial CPU Execution ---\n",
      "Serial CPU Execution Time: 23.7251 seconds\n",
      "\n",
      "--- Parallel CPU Execution ---\n",
      "Number of CPUs available: 24\n",
      "Parallel CPU Execution Time: 27.9879 seconds\n",
      "\n",
      "--- Serial GPU Execution ---\n",
      "Using GPU: NVIDIA A100 80GB PCIe\n",
      "Serial GPU Execution Time: 23.3915 seconds\n",
      "\n",
      "--- Parallel GPU Execution ---\n",
      "Number of GPUs available: 1\n",
      "Parallel GPU Execution Time: 23.7863 seconds\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings from PyTorch\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "size_of_task = 2000000  # Increase the size of the computation task\n",
    "\n",
    "print(\"--- Serial CPU Execution ---\")\n",
    "serial_cpu(size_of_task)\n",
    "\n",
    "print(\"\\n--- Parallel CPU Execution ---\")\n",
    "parallel_cpu(size_of_task)\n",
    "\n",
    "print(\"\\n--- Serial GPU Execution ---\")\n",
    "serial_gpu(size_of_task)\n",
    "\n",
    "print(\"\\n--- Parallel GPU Execution ---\")\n",
    "parallel_gpu(size_of_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39575256-9888-43f4-8d79-f071d2345698",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Results analysis\n",
    "\n",
    "**Serial CPU Execution:** this execution time represents the baseline performance of running the intensive computation sequentially on a single CPU core.\n",
    "\n",
    "**Parallel CPU Execution**: Utilizing Python's `multiprocessing.Pool`, the workload is distributed among all available CPU cores (`mp.cpu_count()`). This parallel approach demonstrates faster execution compared to the serial CPU execution due to concurrent processing.\n",
    "\n",
    "**Serial GPU Execution**: The computation is performed on a single GPU. Its execution time is much lower than the one of Serial CPU, highlighting the GPU's parallel processing capabilities.\n",
    "\n",
    "**Parallel GPU Execution**: Parallel execution across multiple GPUs showcases significant speedup compared to serial and parallel CPU and serial GPU executions. This highlights the advantage of leveraging multiple GPUs for parallel processing tasks.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we observed that parallel execution on multiple CPU cores (`Parallel CPU`) outperformed serial GPU (`Serial GPU`), serial CPU (`Serial CPU`) and parallel CPU (`Parallel CPU`) executions for the given workload. This emphasizes the efficiency of utilizing multiple CPU cores.\n",
    "\n",
    "Further optimization and tuning of workload distribution can potentially enhance the performance of both CPU and GPU executions based on specific computational requirements and hardware capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a8e81-925a-46c6-9e70-a1ba447d15e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
