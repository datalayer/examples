{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7669d2f8-3220-4090-a4fe-3609cdbdf4b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T16:56:04.238059Z",
     "iopub.status.busy": "2024-03-22T16:56:04.237758Z",
     "iopub.status.idle": "2024-03-22T16:56:04.362371Z",
     "shell.execute_reply": "2024-03-22T16:56:04.361837Z",
     "shell.execute_reply.started": "2024-03-22T16:56:04.238033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echarles\n",
      "Q: How are you? A: \n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "prompt=\"Q: How are you? A: \"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f2039b-82a4-461b-9617-82d7e3260e4f",
   "metadata": {
    "datalayer": {
     "in": [
      "prompt"
     ],
     "kernel": {
      "displayName": "Llama CPP GPU (CUDA) Environment",
      "language": "python",
      "location": "remote",
      "name": "llama-cpp-cuda-env",
      "params": {
       "persistent": false
      }
     },
     "out": "text"
    },
    "execution": {
     "iopub.execute_input": "2024-03-22T16:56:37.572694Z",
     "iopub.status.busy": "2024-03-22T16:56:37.572210Z",
     "iopub.status.idle": "2024-03-22T16:56:40.610177Z",
     "shell.execute_reply": "2024-03-22T16:56:40.609426Z",
     "shell.execute_reply.started": "2024-03-22T16:56:37.572668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter-01hskh50g4y5kvjdd96hwntzd7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: Tesla V100S-PCIE-32GB, compute capability 7.0, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How are you? A:  I am fine B:  I'm not sure C:  I'm really sorry\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"Qwen/Qwen1.5-0.5B-Chat-GGUF\",\n",
    "    n_gpu_layers=3,\n",
    "    filename=\"*q8_0.gguf\",\n",
    "    verbose=False\n",
    ")\n",
    "output = llm(\n",
    "    prompt, # Prompt\n",
    "    max_tokens=None, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "    stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "    echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "\n",
    "text=output[\"choices\"][0][\"text\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d8091e6-8e5b-45e3-9e47-b86776377d27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T16:56:49.295081Z",
     "iopub.status.busy": "2024-03-22T16:56:49.294846Z",
     "iopub.status.idle": "2024-03-22T16:56:49.406443Z",
     "shell.execute_reply": "2024-03-22T16:56:49.405995Z",
     "shell.execute_reply.started": "2024-03-22T16:56:49.295065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echarles\n",
      "Q: How are you? A:  I am fine B:  I'm not sure C:  I'm really sorry\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54366a50-f4c4-4a96-b0c7-402d96df3013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
