{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Instruction tuning Mistral 7B on Alpaca dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to fine-tune [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) using the [Alapca dataset](https://huggingface.co/datasets/tatsu-lab/alpaca). Mistral 7B is a large language model (LLM) that contains 7.3 billion parameters and is one of the most powerful models for its size. However, this base model is not instruction-tuned, meaning it may struggle to follow instructions and perform specific tasks.\n",
    "\n",
    "The Alpaca dataset consists of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine. These can be used for instruction tuning, helping language models to better understand and follow instructions. By fine-tuning Mistral 7B on the Alpaca dataset, the model will significantly improve its capabilities to perform tasks such as conversation and answering questions accurately.\n",
    "\n",
    "We will utilize [torchtune](https://github.com/pytorch/torchtune), a PyTorch-native library designed to facilitate experimentation with LLMs, for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T11:12:22.185992Z",
     "iopub.status.busy": "2024-04-21T11:12:22.185714Z",
     "iopub.status.idle": "2024-04-21T11:12:24.058491Z",
     "shell.execute_reply": "2024-04-21T11:12:24.057222Z",
     "shell.execute_reply.started": "2024-04-21T11:12:22.185962Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtune in /opt/conda/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (from torchtune) (2.19.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (from torchtune) (0.22.2)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from torchtune) (0.4.3)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.11/site-packages (from torchtune) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.11/site-packages (from torchtune) (0.6.0)\n",
      "Requirement already satisfied: blobfile>=2 in /opt/conda/lib/python3.11/site-packages (from torchtune) (2.1.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from torchtune) (4.66.2)\n",
      "Requirement already satisfied: omegaconf in /opt/conda/lib/python3.11/site-packages (from torchtune) (2.3.0)\n",
      "Requirement already satisfied: torchao==0.1 in /opt/conda/lib/python3.11/site-packages (from torchtune) (0.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from torchao==0.1->torchtune) (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchao==0.1->torchtune) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from torchao==0.1->torchtune) (24.0)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in /opt/conda/lib/python3.11/site-packages (from blobfile>=2->torchtune) (3.20.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /opt/conda/lib/python3.11/site-packages (from blobfile>=2->torchtune) (2.2.1)\n",
      "Requirement already satisfied: lxml~=4.9 in /opt/conda/lib/python3.11/site-packages (from blobfile>=2->torchtune) (4.9.4)\n",
      "Requirement already satisfied: filelock~=3.0 in /opt/conda/lib/python3.11/site-packages (from blobfile>=2->torchtune) (3.13.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (15.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets->torchtune) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets->torchtune) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->torchtune) (4.10.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.11/site-packages (from omegaconf->torchtune) (4.9.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken->torchtune) (2024.4.16)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->torchtune) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->torchtune) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->torchtune) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->torchtune) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->torchtune) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets->torchtune) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets->torchtune) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets->torchtune) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->torchtune) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->torchtune) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->torchtune) (2024.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch->torchao==0.1->torchtune) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchao==0.1->torchtune) (12.4.127)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->torchao==0.1->torchtune) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch->torchao==0.1->torchtune) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install torchtune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Mistral 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to donwload Mistral 7B. This can be achieved through torchtune with the following cell.\n",
    "\n",
    "> **_NOTE:_** Set your environment variable `<HF_TOKEN>` or pass in --hf-token to the command in order to validate your access. You can find your token at https://huggingface.co/settings/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tune download mistralai/Mistral-7B-v0.1 \\\n",
    "--output-dir ./mistral-7B \\\n",
    "--hf-token <HF_TOKEN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using Mistral 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've downloaded the base model, let's employ it to generate an answer from a text input.\n",
    "\n",
    "When using torchtune, several pieces of information need to be provided in a configuration file, including the type of model to use, its location, and which type of device should be utilized. The cell below generates a *.yaml* file containing all the necessary information to use Mistral 7B for inference on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T08:15:18.131945Z",
     "iopub.status.busy": "2024-04-21T08:15:18.131611Z",
     "iopub.status.idle": "2024-04-21T08:15:18.137808Z",
     "shell.execute_reply": "2024-04-21T08:15:18.137106Z",
     "shell.execute_reply.started": "2024-04-21T08:15:18.131914Z"
    }
   },
   "outputs": [],
   "source": [
    "# mistral_generation.yaml\n",
    "\n",
    "with open(\"mistral_generation.yaml\", \"w\") as fp:\n",
    "    fp.write(\n",
    "        \"\"\"\n",
    "        # Config for running the InferenceRecipe in generate.py to generate output from an LLM\n",
    "        #\n",
    "        # To launch, run the following command from root torchtune directory:\n",
    "        #    tune run generate --config generation\n",
    "        \n",
    "        # Model arguments\n",
    "        model:\n",
    "          _component_: torchtune.models.mistral.mistral_7b\n",
    "        \n",
    "        checkpointer:\n",
    "          _component_: torchtune.utils.FullModelHFCheckpointer\n",
    "          checkpoint_dir: /home/jovyan/content/mistral-7B\n",
    "          checkpoint_files: [\n",
    "            pytorch_model-00001-of-00002.bin,\n",
    "            pytorch_model-00002-of-00002.bin\n",
    "          ]\n",
    "          recipe_checkpoint: null\n",
    "          output_dir: /home/jovyan/content/mistral-7B\n",
    "          model_type: MISTRAL\n",
    "        resume_from_checkpoint: False\n",
    "        \n",
    "        device: cuda\n",
    "        dtype: bf16\n",
    "        \n",
    "        seed: 1234\n",
    "        \n",
    "        # Tokenizer arguments\n",
    "        tokenizer:\n",
    "          _component_: torchtune.models.mistral.mistral_tokenizer\n",
    "          path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
    "        \n",
    "        # Generation arguments; defaults taken from gpt-fast\n",
    "        prompt: \"Hello, my name is\"\n",
    "        max_new_tokens: 300\n",
    "        temperature: 0.6 # 0.8 and 0.6 are popular values to try\n",
    "        top_k: 300\n",
    "        \n",
    "        quantizer: null\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to get an answer from Mistral 7B to the following prompt:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "### Input:\n",
    "Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n",
    "\n",
    "### Context:\n",
    "CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T08:15:18.139272Z",
     "iopub.status.busy": "2024-04-21T08:15:18.138992Z",
     "iopub.status.idle": "2024-04-21T08:15:43.984082Z",
     "shell.execute_reply": "2024-04-21T08:15:43.982967Z",
     "shell.execute_reply.started": "2024-04-21T08:15:18.139250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils.logging:Running InferenceRecipe with resolved config:\n",
      "\n",
      "checkpointer:\n",
      "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/jovyan/content/mistral-7B\n",
      "  checkpoint_files:\n",
      "  - pytorch_model-00001-of-00002.bin\n",
      "  - pytorch_model-00002-of-00002.bin\n",
      "  model_type: MISTRAL\n",
      "  output_dir: /home/jovyan/content/mistral-7B\n",
      "  recipe_checkpoint: null\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "max_new_tokens: 300\n",
      "model:\n",
      "  _component_: torchtune.models.mistral.mistral_7b\n",
      "prompt: You are a powerful text-to-SQL model. Your job is to answer questions about\n",
      "  a database. You are given a question and context regarding one or more tables.\\n\\nYou\n",
      "  must output the SQL query that answers the question.\\n### Input:\\nWhich Class has\n",
      "  a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\\n\\n###\n",
      "  Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license\n",
      "  VARCHAR)\\n\\n### Response:\\n\n",
      "quantizer: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 1234\n",
      "temperature: 0.6\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.mistral.mistral_tokenizer\n",
      "  path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
      "top_k: 300\n",
      "\n",
      "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
      "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
      "INFO:torchtune.utils.logging:You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\nYou must output the SQL query that answers the question.\\n### Input:\\nWhich Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\\n\\n### Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\\n\\n### Response:\\nSELECT class, frequency_mhz, city_of_license FROM table_name_12 WHERE frequency_mhz > '91.5' AND city_of_license = 'hyannis, nebraska'\\n\\n### Input:\\nWhat is the City of license of the Class with the highest frequency Mhz?\\n\\n### Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\\n\\n### Response:\\nSELECT city_of_license FROM table_name_12 WHERE frequency_mhz = (SELECT MAX(frequency_mhz) FROM table_name_12)\\n\\n### Input:\\nWhat is the Class that has a City of license of hyannis, nebraska?\\n\\n### Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\\n\\n### Response:\\nSELECT class FROM table_name_12 WHERE city_of_license = 'hyannis, nebraska'\\n\\n### Input:\\nWhat is the City of license of the Class with the lowest frequency Mhz?\\n\\n### Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mh\n",
      "INFO:torchtune.utils.logging:Time for inference: 19.67 sec total, 15.25 tokens/sec\n",
      "INFO:torchtune.utils.logging:Bandwidth achieved: 491.15 GB/s\n",
      "INFO:torchtune.utils.logging:Memory used: 34.35 GB\n"
     ]
    }
   ],
   "source": [
    "! tune run generate --config ./mistral_generation.yaml \\\n",
    "prompt=\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\nYou must output the SQL query that answers the question.\\n### Input:\\nWhich Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\\n\\n### Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\\n\\n### Response:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, using the base model, the answer is not exactly what we were hoping for. This is because the objective of the model is next word prediction.\n",
    "\n",
    "To improve the model's understanding, we will fine tune it using the Alpaca dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we need to create a configuration file that holds the relevant information to fine tune the model.\n",
    "\n",
    "LoRA (Low-Rank Adaptation), a highly efficient method of LLM fine tuning, is here used via this configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T08:16:45.799756Z",
     "iopub.status.busy": "2024-04-21T08:16:45.799372Z",
     "iopub.status.idle": "2024-04-21T08:16:45.806747Z",
     "shell.execute_reply": "2024-04-21T08:16:45.806066Z",
     "shell.execute_reply.started": "2024-04-21T08:16:45.799724Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7B_lora_single_device_mistral.yaml\n",
    "\n",
    "with open(\"7B_lora_single_device_mistral.yaml\", \"w\") as fp:\n",
    "    fp.write(\n",
    "        \"\"\"\n",
    "        # Tokenizer\n",
    "        tokenizer:\n",
    "          _component_: torchtune.models.mistral.mistral_tokenizer\n",
    "          path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
    "        \n",
    "        # Dataset\n",
    "        dataset:\n",
    "          _component_: torchtune.datasets.alpaca_dataset\n",
    "          train_on_input: True\n",
    "        seed: null\n",
    "        shuffle: True\n",
    "        \n",
    "        # Model Arguments\n",
    "        model:\n",
    "          _component_: torchtune.models.mistral.lora_mistral_7b\n",
    "          lora_attn_modules: ['q_proj', 'k_proj', 'v_proj']\n",
    "          apply_lora_to_mlp: True\n",
    "          apply_lora_to_output: True\n",
    "          lora_rank: 64\n",
    "          lora_alpha: 16\n",
    "        \n",
    "        checkpointer:\n",
    "          _component_: torchtune.utils.FullModelHFCheckpointer\n",
    "          checkpoint_dir: /home/jovyan/content/mistral-7B\n",
    "          checkpoint_files: [\n",
    "            pytorch_model-00001-of-00002.bin,\n",
    "            pytorch_model-00002-of-00002.bin\n",
    "          ]\n",
    "          recipe_checkpoint: null\n",
    "          output_dir: /home/jovyan/content/mistral-7B\n",
    "          model_type: MISTRAL\n",
    "        resume_from_checkpoint: False\n",
    "        \n",
    "        optimizer:\n",
    "          _component_: torch.optim.AdamW\n",
    "          lr: 2e-5\n",
    "        \n",
    "        lr_scheduler:\n",
    "          _component_: torchtune.modules.get_cosine_schedule_with_warmup\n",
    "          num_warmup_steps: 100\n",
    "        \n",
    "        loss:\n",
    "          _component_: torch.nn.CrossEntropyLoss\n",
    "        \n",
    "        # Fine-tuning arguments\n",
    "        batch_size: 4\n",
    "        epochs: 3\n",
    "        max_steps_per_epoch: null\n",
    "        gradient_accumulation_steps: 4\n",
    "        compile: False\n",
    "        \n",
    "        # Training env\n",
    "        device: cuda\n",
    "        \n",
    "        # Memory management\n",
    "        enable_activation_checkpointing: True\n",
    "        \n",
    "        # Reduced precision\n",
    "        dtype: bf16\n",
    "        \n",
    "        # Logging\n",
    "        metric_logger:\n",
    "          _component_: torchtune.utils.metric_logging.DiskLogger\n",
    "          log_dir: ${output_dir}\n",
    "        output_dir: /home/jovyan/content/mistral-7B\n",
    "        log_every_n_steps: null\n",
    "        \n",
    "        # Show case the usage of pytorch profiler\n",
    "        # Set enabled to False as it's only needed for debugging training\n",
    "        profiler:\n",
    "          _component_: torchtune.utils.profiler\n",
    "          enabled: False\n",
    "          output_dir: /home/jovyan/content/mistral-7B/torchtune_perf_tracing.json\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T08:16:51.180012Z",
     "iopub.status.busy": "2024-04-21T08:16:51.179684Z",
     "iopub.status.idle": "2024-04-21T11:12:22.184090Z",
     "shell.execute_reply": "2024-04-21T11:12:22.182938Z",
     "shell.execute_reply.started": "2024-04-21T08:16:51.179989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils.logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/jovyan/content/mistral-7B\n",
      "  checkpoint_files:\n",
      "  - pytorch_model-00001-of-00002.bin\n",
      "  - pytorch_model-00002-of-00002.bin\n",
      "  model_type: MISTRAL\n",
      "  output_dir: /home/jovyan/content/mistral-7B\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "  train_on_input: true\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "epochs: 3\n",
      "gradient_accumulation_steps: 4\n",
      "log_every_n_steps: null\n",
      "loss:\n",
      "  _component_: torch.nn.CrossEntropyLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.modules.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 100\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.utils.metric_logging.DiskLogger\n",
      "  log_dir: /home/jovyan/content/mistral-7B\n",
      "model:\n",
      "  _component_: torchtune.models.mistral.lora_mistral_7b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: true\n",
      "  lora_alpha: 16\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  lora_rank: 64\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  lr: 2.0e-05\n",
      "output_dir: /home/jovyan/content/mistral-7B\n",
      "profiler:\n",
      "  _component_: torchtune.utils.profiler\n",
      "  enabled: false\n",
      "  output_dir: /home/jovyan/content/mistral-7B/torchtune_perf_tracing.json\n",
      "resume_from_checkpoint: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.mistral.mistral_tokenizer\n",
      "  path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
      "\n",
      "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 1421891387. Local seed is seed + rank = 1421891387 + 0\n",
      "Writing logs to /home/jovyan/content/mistral-7B/log_1713687413.txt\n",
      "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
      "INFO:torchtune.utils.logging:Memory Stats after model init:\n",
      "{'peak_memory_active': 15.804604928, 'peak_memory_alloc': 15.804604928, 'peak_memory_reserved': 15.827206144}\n",
      "INFO:torchtune.utils.logging:Tokenizer is initialized from file.\n",
      "INFO:torchtune.utils.logging:Optimizer and loss are initialized.\n",
      "INFO:torchtune.utils.logging:Loss is initialized.\n",
      "Downloading readme: 100%|██████████████████| 7.47k/7.47k [00:00<00:00, 14.8MB/s]\n",
      "Downloading data: 100%|████████████████████| 24.2M/24.2M [00:00<00:00, 57.6MB/s]\n",
      "Generating train split: 100%|██| 52002/52002 [00:00<00:00, 238912.65 examples/s]\n",
      "INFO:torchtune.utils.logging:Dataset and Sampler are initialized.\n",
      "INFO:torchtune.utils.logging:Learning rate scheduler is initialized.\n",
      "1|13001|Loss: 1.153263807296753: 100%|████| 13001/13001 [57:42<00:00,  3.75it/s]\n",
      "INFO:torchtune.utils.logging:Model checkpoint of size 9.94 GB saved to /home/jovyan/content/mistral-7B/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils.logging:Model checkpoint of size 4.54 GB saved to /home/jovyan/content/mistral-7B/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils.logging:Adapter checkpoint of size 0.31 GB saved to /home/jovyan/content/mistral-7B/adapter_0.pt\n",
      "INFO:torchtune.utils.logging:Recipe checkpoint of size 0.61 GB saved to /home/jovyan/content/mistral-7B/recipe_state.pt\n",
      "2|13001|Loss: 0.5868340730667114: 100%|███| 13001/13001 [58:07<00:00,  3.73it/s]\n",
      "INFO:torchtune.utils.logging:Model checkpoint of size 9.94 GB saved to /home/jovyan/content/mistral-7B/hf_model_0001_1.pt\n",
      "INFO:torchtune.utils.logging:Model checkpoint of size 4.54 GB saved to /home/jovyan/content/mistral-7B/hf_model_0002_1.pt\n",
      "INFO:torchtune.utils.logging:Adapter checkpoint of size 0.31 GB saved to /home/jovyan/content/mistral-7B/adapter_1.pt\n",
      "INFO:torchtune.utils.logging:Recipe checkpoint of size 0.61 GB saved to /home/jovyan/content/mistral-7B/recipe_state.pt\n",
      "3|13001|Loss: 0.669356644153595: 100%|████| 13001/13001 [58:04<00:00,  3.73it/s]\n",
      "INFO:torchtune.utils.logging:Model checkpoint of size 9.94 GB saved to /home/jovyan/content/mistral-7B/hf_model_0001_2.pt\n",
      "INFO:torchtune.utils.logging:Model checkpoint of size 4.54 GB saved to /home/jovyan/content/mistral-7B/hf_model_0002_2.pt\n",
      "INFO:torchtune.utils.logging:Adapter checkpoint of size 0.31 GB saved to /home/jovyan/content/mistral-7B/adapter_2.pt\n"
     ]
    }
   ],
   "source": [
    "! tune run lora_finetune_single_device --config ./7B_lora_single_device_mistral.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a NVIDIA H100 PCIe GPU, one epoch is completed in roughly 58 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using Mistral 7B fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, there is the configuration file that will be used to infere with the fine tuned version of the Mistral 7B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T11:14:14.921836Z",
     "iopub.status.busy": "2024-04-21T11:14:14.920827Z",
     "iopub.status.idle": "2024-04-21T11:14:14.927525Z",
     "shell.execute_reply": "2024-04-21T11:14:14.926871Z",
     "shell.execute_reply.started": "2024-04-21T11:14:14.921798Z"
    }
   },
   "outputs": [],
   "source": [
    "# mistral_fine-tuned_generation.yaml\n",
    "\n",
    "with open(\"mistral_fine-tuned_generation.yaml\", \"w\") as fp:\n",
    "    fp.write(\n",
    "        \"\"\"\n",
    "        # Config for running the InferenceRecipe in generate.py to generate output from an LLM\n",
    "        #\n",
    "        # To launch, run the following command from root torchtune directory:\n",
    "        #    tune run generate --config generation\n",
    "        \n",
    "        # Model arguments\n",
    "        model:\n",
    "          _component_: torchtune.models.mistral.mistral_7b\n",
    "        \n",
    "        checkpointer:\n",
    "          _component_: torchtune.utils.FullModelHFCheckpointer\n",
    "          checkpoint_dir: /home/jovyan/content/mistral-7B\n",
    "          checkpoint_files: [\n",
    "            hf_model_0001_2.pt,\n",
    "            hf_model_0002_2.pt,\n",
    "          ]\n",
    "          recipe_checkpoint: null\n",
    "          output_dir: /home/jovyan/content/mistral-7B\n",
    "          model_type: MISTRAL\n",
    "        resume_from_checkpoint: False\n",
    "        \n",
    "        device: cuda\n",
    "        dtype: bf16\n",
    "        \n",
    "        seed: 1234\n",
    "        \n",
    "        # Tokenizer arguments\n",
    "        tokenizer:\n",
    "          _component_: torchtune.models.mistral.mistral_tokenizer\n",
    "          path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
    "        \n",
    "        # Generation arguments; defaults taken from gpt-fast\n",
    "        prompt: \"Hello, my name is\"\n",
    "        max_new_tokens: 300\n",
    "        temperature: 0.6 # 0.8 and 0.6 are popular values to try\n",
    "        top_k: 300\n",
    "\n",
    "        quantizer: null\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the exact same prompt as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T11:14:15.476034Z",
     "iopub.status.busy": "2024-04-21T11:14:15.474639Z",
     "iopub.status.idle": "2024-04-21T11:14:24.899906Z",
     "shell.execute_reply": "2024-04-21T11:14:24.899115Z",
     "shell.execute_reply.started": "2024-04-21T11:14:15.475991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils.logging:Running InferenceRecipe with resolved config:\n",
      "\n",
      "checkpointer:\n",
      "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/jovyan/content/mistral-7B\n",
      "  checkpoint_files:\n",
      "  - hf_model_0001_2.pt\n",
      "  - hf_model_0002_2.pt\n",
      "  model_type: MISTRAL\n",
      "  output_dir: /home/jovyan/content/mistral-7B\n",
      "  recipe_checkpoint: null\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "max_new_tokens: 300\n",
      "model:\n",
      "  _component_: torchtune.models.mistral.mistral_7b\n",
      "prompt: You are a powerful text-to-SQL model. Your job is to answer questions about\n",
      "  a database. You are given a question and context regarding one or more tables.\\n\\nYou\n",
      "  must output the SQL query that answers the question.\\n### Input:\\nWhich Class has\n",
      "  a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\\n\\n###\n",
      "  Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license\n",
      "  VARCHAR)\\n\\n### Response:\\n\n",
      "quantizer: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 1234\n",
      "temperature: 0.6\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.mistral.mistral_tokenizer\n",
      "  path: /home/jovyan/content/mistral-7B/tokenizer.model\n",
      "top_k: 300\n",
      "\n",
      "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
      "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
      "INFO:torchtune.utils.logging:You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\nYou must output the SQL query that answers the question.\\n### Input:\\nWhich Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\\n\\n### Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\\n\\n### Response:\\nSELECT class, frequency_mhz, city_of_license FROM table_name_12 WHERE frequency_mhz > 91.5 AND city_of_license = 'hyannis, nebraska'\n",
      "INFO:torchtune.utils.logging:Time for inference: 3.43 sec total, 14.57 tokens/sec\n",
      "INFO:torchtune.utils.logging:Bandwidth achieved: 469.23 GB/s\n",
      "INFO:torchtune.utils.logging:Memory used: 34.35 GB\n"
     ]
    }
   ],
   "source": [
    "! tune run generate --config ./mistral_fine-tuned_generation.yaml \\\n",
    "prompt=\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\nYou must output the SQL query that answers the question.\\n### Input:\\nWhich Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\\n\\n### Context:\\nCREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\\n\\n### Response:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the output is more relevant, the model only outputs the answer to our question. The fine tuning process has worked!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
