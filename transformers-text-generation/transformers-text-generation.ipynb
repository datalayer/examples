{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to do text generation with LLMs using the Hugging Face Transformers library. We'll use two different versions of the Gemma-7b model. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma-7b-it is the instruct fine-tuned version of Gemma-7b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T11:01:15.616099Z",
     "iopub.status.busy": "2024-06-27T11:01:15.614003Z",
     "iopub.status.idle": "2024-06-27T11:01:20.722171Z",
     "shell.execute_reply": "2024-06-27T11:01:20.720709Z",
     "shell.execute_reply.started": "2024-06-27T11:01:15.616022Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_** Pass your HF_TOKEN in cell below in order to validate your access. You can find your token at https://huggingface.co/settings/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T11:02:40.468373Z",
     "iopub.status.busy": "2024-06-27T11:02:40.467112Z",
     "iopub.status.idle": "2024-06-27T11:02:40.578214Z",
     "shell.execute_reply": "2024-06-27T11:02:40.576929Z",
     "shell.execute_reply.started": "2024-06-27T11:02:40.468316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/jovyan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma-7b\n",
    "\n",
    "As expected, using the base model, the answer is not exactly what we were hoping for. This is because the objective of the model is next word prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T11:03:05.487728Z",
     "iopub.status.busy": "2024-06-27T11:03:05.487552Z",
     "iopub.status.idle": "2024-06-27T11:03:34.218724Z",
     "shell.execute_reply": "2024-06-27T11:03:34.217903Z",
     "shell.execute_reply.started": "2024-06-27T11:03:05.487713Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a04de665284aa090e1bb6b37ee5570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "I’m not a poet, but I’ve been thinking about this for a while.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who don’t know what it is.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who do know what it is.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are interested in it, but don’t know where to start.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are already experts in the field.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are not interested in it at all.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are afraid of it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are excited by it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are indifferent to it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are angry about it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are happy about it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are sad about it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are confused by it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are excited by it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are angry about it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are happy about it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are sad about it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are confused by it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are excited by it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are angry about it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are happy about it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are sad about it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are confused by it.\n",
      "\n",
      "I’ve been thinking about how to explain Machine Learning to people who are excited by it.\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-7b\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma-7b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T11:03:34.222020Z",
     "iopub.status.busy": "2024-06-27T11:03:34.221849Z",
     "iopub.status.idle": "2024-06-27T11:03:40.752899Z",
     "shell.execute_reply": "2024-06-27T11:03:40.751767Z",
     "shell.execute_reply.started": "2024-06-27T11:03:34.222004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1dded98dd64b9aa098dc7fa16b9b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-7b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T11:03:40.759776Z",
     "iopub.status.busy": "2024-06-27T11:03:40.759373Z",
     "iopub.status.idle": "2024-06-27T11:03:54.490380Z",
     "shell.execute_reply": "2024-06-27T11:03:54.489418Z",
     "shell.execute_reply.started": "2024-06-27T11:03:40.759739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "In the realm of data, a tale unfolds,\n",
      "Where algorithms dance, stories untold.\n",
      "With patterns hidden, and insights deep,\n",
      "Machine learning blooms, secrets to keep.\n",
      "\n",
      "Data whispers secrets, a hidden treasure,\n",
      "It fuels the engine, a learning pleasure.\n",
      "With algorithms, it takes a flight,\n",
      "Unveiling patterns, shining light.\n",
      "\n",
      "From simple rules to complex trees,\n",
      "Models emerge, with unseen ease.\n",
      "They learn from data, with every bite,\n",
      "And make predictions, with all their might.\n",
      "\n",
      "In the field of medicine, a revolution unfolds,\n",
      "Machine learning guides, stories untold.\n",
      "It diagnoses diseases, with precision,\n",
      "And helps patients find solace in their condition.\n",
      "\n",
      "In finance, it forecasts the future with grace,\n",
      "Unveils market trends, with lightning pace.\n",
      "It optimizes investments, with a keen hand,\n",
      "And guides investors to make a stand.\n",
      "\n",
      "But with power comes responsibility,\n",
      "The potential for bias, a treacherous sea.\n",
      "In the hands of humans, it can be flawed,\n",
      "It's a tool for good, but also a sword.\n",
      "\n",
      "Yet, the future holds promise, bright,\n",
      "As machine learning continues to take flight.\n",
      "With ethical guidance, and a human touch,\n",
      "It can unlock solutions, beyond our clutch.\n",
      "\n",
      "So let us embrace the power of this force,\n",
      "And use it for good, to make a course.\n",
      "In the realm of machine learning, we find,\n",
      "A path to progress, a brighter mind.<eos>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**input_ids, max_new_tokens=2000)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the output is more relevant, the model only outputs the answer to our question. This is because we have used the instruction tuned version of Gemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
