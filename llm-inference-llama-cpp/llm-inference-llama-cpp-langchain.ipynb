{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0988b8",
   "metadata": {},
   "source": [
    "# LLM Inference with Llama.cpp and Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15619177",
   "metadata": {},
   "source": [
    "## Checking GPU Availability\n",
    "\n",
    "Before loading the model, we check if GPU offloading is supported on the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from llama_cpp.llama_cpp import load_shared_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30424dfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T15:09:22.084167Z",
     "iopub.status.busy": "2025-01-29T15:09:22.084001Z",
     "iopub.status.idle": "2025-01-29T15:09:22.089664Z",
     "shell.execute_reply": "2025-01-29T15:09:22.089173Z",
     "shell.execute_reply.started": "2025-01-29T15:09:22.084150Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_gpu_available() -> bool:\n",
    "    lib = load_shared_library('llama',pathlib.Path('/opt/conda/lib/python3.11/site-packages/llama_cpp/lib'))\n",
    "    return bool(lib.llama_supports_gpu_offload())\n",
    "\n",
    "is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de53f9",
   "metadata": {},
   "source": [
    "## Inference with Langchain Llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3ebb6-919f-441b-9f87-c91950c21072",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T15:24:10.334797Z",
     "iopub.status.busy": "2025-01-31T15:24:10.334460Z",
     "iopub.status.idle": "2025-01-31T15:24:10.995673Z",
     "shell.execute_reply": "2025-01-31T15:24:10.995204Z",
     "shell.execute_reply.started": "2025-01-31T15:24:10.334775Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THE FOLLOWING VARIABLES\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "model_path = \"ai-models/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf\"\n",
    "\n",
    "# The number of model's layers to offload to the GPU (if set to -1, all model layers will be offloaded)\n",
    "n_gpu_layers = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4180fd-ee4c-47eb-97f3-4c43d02e4fa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T15:24:10.997030Z",
     "iopub.status.busy": "2025-01-31T15:24:10.996678Z",
     "iopub.status.idle": "2025-01-31T15:24:10.999551Z",
     "shell.execute_reply": "2025-01-31T15:24:10.999050Z",
     "shell.execute_reply.started": "2025-01-31T15:24:10.997014Z"
    }
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Question: {question}.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f6dff-f7c0-4802-bf91-fd81739704bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T15:28:26.266172Z",
     "iopub.status.busy": "2025-01-31T15:28:26.265806Z",
     "iopub.status.idle": "2025-01-31T15:28:31.474054Z",
     "shell.execute_reply": "2025-01-31T15:28:31.473629Z",
     "shell.execute_reply.started": "2025-01-31T15:28:26.266154Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.75,\n",
    "    max_tokens=200,\n",
    "    top_p=1,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36cc8b-d1a7-415e-9808-675dc118ead9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T15:30:26.656973Z",
     "iopub.status.busy": "2025-01-31T15:30:26.656635Z",
     "iopub.status.idle": "2025-01-31T15:31:19.276034Z",
     "shell.execute_reply": "2025-01-31T15:31:19.275595Z",
     "shell.execute_reply.started": "2025-01-31T15:30:26.656956Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "What is Machine Learning?\n",
    "\"\"\"\n",
    "\n",
    "llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5c446-8557-4f24-ba93-43246a5bc2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
